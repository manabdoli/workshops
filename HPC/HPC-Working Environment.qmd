---
title: "HPC Workflow Guide: Development Environment and Data Management"
author: "Mansour Abdoli"
ai.disclosure: "Content is initially generated by Claude but reviewed/tested"
format: html
---


## 1. Recommended Development Setup

### Option A: VS Code with Remote-SSH (RECOMMENDED)
**Best for: Python, R, Julia, general coding**

#### Setup:
```bash
# On your local machine:
# 1. Install VS Code
# 2. Install "Remote - SSH" extension
# 3. Configure SSH connection

# In VS Code, press F1 and select "Remote-SSH: Connect to Host"
# Add your HPC: user@hpc.institution.edu
```

#### SSH Config (~/.ssh/config):
```
Host hpc
    HostName hpc.institution.edu
    User your_username
    ForwardAgent yes
    ServerAliveInterval 60
    ServerAliveCountMax 10
```

#### Essential VS Code Extensions for HPC:
- **Remote - SSH**: Connect to HPC
- **Python**: IntelliSense, debugging
- **R**: R language support
- **Julia**: Julia language support
- **SLURM Syntax Highlighting**: For job scripts
- **GitLens**: Git integration
- **Remote Explorer**: Manage connections

#### Benefits:
- Edit files directly on HPC with full IDE features
- IntelliSense and autocomplete work with HPC libraries
- Integrated terminal for running commands
- Git integration
- Debug Python code remotely
- Split editor for multiple files

---

### Option B: JupyterLab on HPC with SSH Tunneling
**Best for: Interactive Python/R/Julia work, data exploration**

#### Setup JupyterLab on HPC:
```bash
# On HPC, in your virtual environment
pip install jupyterlab jupyter-server-proxy

# Or load HPC module
module load jupyterlab
```

#### Create Launch Script (~/launch_jupyter.sh):
```bash
#!/bin/bash
#SBATCH --job-name=jupyter
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=16G
#SBATCH --time=08:00:00
#SBATCH --output=jupyter_%j.out

# Get the compute node name
NODE=$(hostname -s)
PORT=8888

echo "Starting Jupyter on $NODE:$PORT"
echo "SSH tunnel command:"
echo "ssh -N -L ${PORT}:${NODE}:${PORT} ${USER}@hpc.institution.edu"

# Activate environment
source ~/hpc_test_env/bin/activate

# Start Jupyter
jupyter lab --no-browser --port=${PORT} --ip=${NODE}
```

#### Usage:
```bash
# Submit job
sbatch launch_jupyter.sh

# Check output for connection info
cat jupyter_<job_id>.out

# On local machine, create tunnel (use the command from output)
ssh -N -L 8888:compute-node-15:8888 user@hpc.institution.edu

# Open browser to: localhost:8888
```

---

### Option C: Terminal-Based Editors
**Best for: Quick edits, job scripts, when graphical interface unavailable**

#### Vim (Learning curve, but powerful):
```bash
# Essential Vim commands
vim filename.py
i               # Enter insert mode
ESC             # Exit insert mode
:w              # Save
:q              # Quit
:wq             # Save and quit
dd              # Delete line
yy              # Copy line
p               # Paste
/search         # Search
:set number     # Show line numbers
```

#### Nano (Easier for beginners):
```bash
nano filename.py
# Ctrl+O: Save
# Ctrl+X: Exit
# Ctrl+W: Search
# Ctrl+K: Cut line
# Ctrl+U: Paste
```

#### Basic .vimrc for HPC (~/.vimrc):
```vim
syntax on
set number
set tabstop=4
set shiftwidth=4
set expandtab
set autoindent
colorscheme desert
```

---

## 2. Git Workflow for Code (Not Data)

### Setup on HPC:
```bash
# Configure Git
git config --global user.name "Your Name"
git config --global user.email "your.email@domain.com"

# Setup SSH key for GitHub
ssh-keygen -t ed25519 -C "your.email@domain.com"
cat ~/.ssh/id_ed25519.pub
# Copy this key to GitHub: Settings > SSH and GPG keys
```

### Recommended Repository Structure:
```
project/
├── .gitignore           # Exclude data and results
├── README.md
├── environment.yml      # Conda environment
├── requirements.txt     # Python packages
├── scripts/
│   ├── preprocessing.py
│   ├── analysis.py
│   └── visualization.R
├── slurm_jobs/
│   ├── submit_preprocess.sh
│   └── submit_analysis.sh
├── notebooks/          # Jupyter notebooks
│   └── exploration.ipynb
└── config/
    └── parameters.yaml
```

### Essential .gitignore:
```
# Data files - DO NOT COMMIT
*.csv
*.tsv
*.h5
*.hdf5
*.npy
*.npz
*.rds
*.RData
*.mat
data/
raw_data/
processed_data/
results/
outputs/

# Large model files
*.pkl
*.joblib
*.pth
*.pt
*.ckpt
models/

# Job outputs
*.out
*.err
slurm-*.out

# Environment
__pycache__/
*.pyc
.ipynb_checkpoints/
.venv/
env/
```

### Typical Git Workflow:
```bash
# Clone on HPC
git clone git@github.com:username/project.git
cd project

# Make changes
vim scripts/analysis.py

# Commit and push
git add scripts/analysis.py
git commit -m "Update analysis script"
git push

# Pull changes from another machine
git pull
```

---

## 3. Data Transfer Methods

### Method 1: rsync (BEST for large datasets)
**Use for: Initial data upload, incremental updates**

```bash
# Basic usage - local to HPC
rsync -avzP /local/path/to/data/ user@hpc.institution.edu:~/project/data/

# HPC to local
rsync -avzP user@hpc.institution.edu:~/project/results/ /local/results/

# With exclusions
rsync -avzP --exclude='*.tmp' --exclude='cache/' /local/data/ user@hpc:~/data/

# Resume interrupted transfer
rsync -avzP --partial /local/data/ user@hpc:~/data/

# Dry run (see what would be transferred)
rsync -avzPn /local/data/ user@hpc:~/data/
```

**Flags:**
- `-a`: Archive mode (preserves permissions, timestamps)
- `-v`: Verbose
- `-z`: Compress during transfer
- `-P`: Show progress and keep partial files
- `--partial`: Resume interrupted transfers

---

### Method 2: scp (Simple, one-time transfers)
```bash
# File to HPC
scp /local/file.csv user@hpc.institution.edu:~/data/

# Directory to HPC
scp -r /local/directory/ user@hpc:~/data/

# File from HPC
scp user@hpc:~/results/output.csv /local/results/

# Multiple files
scp user@hpc:~/results/*.csv /local/results/
```

---

### Method 3: Globus (BEST for HUGE datasets, multi-TB)
**Use when: Transferring 100GB+, need reliability**

```bash
# Setup Globus Connect Personal on your machine
# Setup Globus endpoint on HPC (usually pre-configured)
# Use web interface: https://www.globus.org/

# Benefits:
# - Automatic retry on failure
# - Transfer in background
# - Very fast for large files
# - Can schedule transfers
```

---

### Method 4: SSHFS (Mount HPC as local drive)
**Use for: Browsing files, small edits, not recommended for large transfers**

```bash
# Install SSHFS
# macOS: brew install macfuse sshfs
# Linux: sudo apt-get install sshfs

# Mount HPC directory
mkdir ~/hpc_mount
sshfs user@hpc.institution.edu:/home/user ~/hpc_mount

# Unmount
umount ~/hpc_mount
```

---

### Method 5: rclone (Cloud storage integration)
**Use for: Backing up to cloud, sharing with collaborators**

```bash
# Install rclone
# Configure with cloud provider (Google Drive, Dropbox, S3, etc.)
rclone config

# Sync HPC data to cloud
rclone sync ~/project/results remote:project_backup

# From cloud to HPC
rclone sync remote:shared_dataset ~/data/
```

---

## 4. Data Management Best Practices

### Directory Structure on HPC:
```
~/
├── projects/
│   ├── project1/
│   │   ├── scripts/          # Git-tracked
│   │   ├── data/
│   │   │   ├── raw/          # Original, read-only
│   │   │   └── processed/    # Derived data
│   │   ├── results/
│   │   └── logs/
│   └── project2/
├── software/                  # Custom installations
├── scratch/                   # Temporary, fast storage
└── archive/                   # Long-term storage
```

### Storage Tiers (typical HPC):
1. **Home directory** (~50-100GB): Code, small files, personal config
2. **Scratch/work** (1-5TB, purged): Active computation, temporary files
3. **Project storage** (TB+): Shared data, longer retention
4. **Archive/tape** (unlimited, slow): Long-term backup

### Data Workflow:
```bash
# 1. Upload raw data ONCE
rsync -avzP /local/raw_data/ hpc:~/project/data/raw/

# 2. Make raw data read-only
chmod -R a-w ~/project/data/raw/

# 3. Process data in scratch (fast filesystem)
cp -r ~/project/data/raw/* /scratch/$USER/project/
# Run processing
# Move results back
mv /scratch/$USER/project/processed/* ~/project/data/processed/

# 4. Download only what you need
rsync -avzP hpc:~/project/results/summary.csv /local/

# 5. Archive completed projects
tar -czf project1_archive.tar.gz project1/
mv project1_archive.tar.gz ~/archive/
```

---

## 5. Complete Recommended Workflow

### Initial Setup:
```bash
# 1. Setup SSH config for easy connection
vim ~/.ssh/config  # Add HPC config

# 2. Generate and add SSH key to GitHub
ssh-keygen -t ed25519
cat ~/.ssh/id_ed25519.pub  # Add to GitHub

# 3. Test connection
ssh hpc
```

### Daily Development Cycle:

**Option A: VS Code Remote (RECOMMENDED)**
1. Open VS Code
2. Connect to HPC via Remote-SSH
3. Edit code directly on HPC
4. Test in integrated terminal
5. Commit and push to GitHub
6. Submit SLURM jobs

**Option B: Local Development + Sync**
1. Develop code locally in VS Code/PyCharm
2. Commit to GitHub
3. SSH to HPC
4. Pull changes: `git pull`
5. Submit jobs
6. Sync results back: `rsync`

### For Large Data:
```bash
# Upload once
rsync -avzP --partial /local/big_dataset/ hpc:~/data/raw/

# Code changes via Git
git push  # from local
ssh hpc
git pull  # on HPC

# Download only results
rsync -avzP hpc:~/results/figures/ /local/figures/
```

---

## 6. Pro Tips

### Persistent Sessions with tmux:
```bash
# Start tmux session
tmux new -s work

# Detach: Ctrl+b, then d
# Reattach later
tmux attach -t work

# Lost connection? Your session is still running!
```

### Quick File Preview:
```bash
# First few lines
head -n 20 data.csv

# Last few lines
tail -n 20 results.csv

# Count lines
wc -l data.csv

# Search in files
grep "error" *.log

# Disk usage
du -sh */
```

### Automation Script (~/bin/sync_project.sh):
```bash
#!/bin/bash
PROJECT=$1

# Sync code from GitHub
cd ~/projects/$PROJECT
git pull

# Sync results to local
rsync -avzP hpc:~/projects/$PROJECT/results/ \
    /local/projects/$PROJECT/results/

echo "Sync complete!"
```

---

## 7. Troubleshooting

### Connection Issues:
```bash
# Test connection
ssh -v hpc

# Check for stale connections
rm ~/.ssh/known_hosts  # If host key changed

# VS Code won't connect
# Delete: ~/.vscode-server/ on HPC
```

### Transfer Issues:
```bash
# Check disk space on HPC
df -h ~

# Check quota
quota -s

# Find large files
du -ah ~ | sort -rh | head -20
```

---

## Summary: Best Setup

**For most users:**
- **Code**: VS Code with Remote-SSH extension
- **Version control**: GitHub for code only
- **Data transfer**: rsync for datasets, Globus for 100GB+
- **Interactive work**: JupyterLab with SSH tunnel
- **Persistence**: tmux for long-running sessions

**File strategy:**
- Git: Scripts, notebooks, configs
- rsync: Raw data (once), results (as needed)
- Never commit: Data files, model checkpoints, job outputs

This setup gives you a modern IDE experience while working efficiently with HPC resources!

# The Best Setup for Most HPC Users:

## 1. VS Code with Remote-SSH (Primary development)
This is a game-changer. You get:

- Full IDE features while editing files directly on the HPC
- No need to sync code manually
- Integrated terminal
- IntelliSense that understands HPC libraries
- Git integration

## 2. Git for Code Only

- Keep scripts, notebooks, and configs in GitHub
- Never commit data files (use .gitignore)
- Perfect for version control and collaboration

## 3. rsync for Data Transfer

- One-time upload of large datasets
- Incremental syncing of results back to local
- Can resume interrupted transfers
- Much better than scp for anything over 100MB

## 4. JupyterLab + SSH Tunnel (Optional, for interactive work)
Great for exploratory analysis when you want that notebook experience
Quick Start to Get Rolling:
```{bash}
# 1. Install VS Code + Remote-SSH extension
# 2. Configure SSH (~/.ssh/config)
# 3. Connect to HPC in VS Code
# 4. Upload your data once with rsync
# 5. Clone your code repo on HPC
# 6. Start coding!
```

Typical Daily Workflow:

- Open VS Code → Connect to HPC
- Edit files directly on HPC (no syncing!)
- Test in terminal
- Commit to Git
- Submit SLURM jobs
- Download results with rsync when done

The key insight: Separate code (small, version controlled) from data (large, transferred as needed). You'll never accidentally try to commit 50GB to GitHub this way!
